\chapter{Supplementary materials for all chapters}
\label{chap:allsup}

\section{Thesis git and HDD archives}
\label{sec:archive}
The resources used to compile and typeset this document are available at \url{https://github.com/mmattocks/Thesis}. These include most observational datasets referred to in figures. Most of the code used to generate figures, available in \autoref{analysiscode}, can be executed without additional resources, beyond the package dependencies included by the script. These are easily installed in Julia, as one may \path{]add} packages from the interpreter. The scripts may then be \path{include()}d, with appropriate changes to paths where required (normally declared at the top of the script). Not included in the git archive are raw confocal microscopy stacks from any chapter, nor the bioinformatic data from \autoref{chap:rys}), due to the large size of these datasets. The fully calculated ensemble posterior samples are also not included, although they can be replicated from the analysis scripts. These larger datasets are available in the HDD data archive, available in the Tropepe lab at the Department of Cell and Systems Biology, kept with Dr. Vince Tropepe. The git archive will host the most up-to-date and accurate version of the thesis.

\section{Code notes}
Most of the code used in this thesis is presented in \autoref{chap:code}. Three languages are represented: C++ for the CHASTE simulators presented in \autoref{chap:SMME}, Python for the SPSA optimization algorithm and general scripting in \autoref{chap:SMME}, as well as for the generation of \autoref{nucgendist} in \autoref{chap:rys}, and Julia for everything else. The Julia code is by far the easiest to execute. We have found it to be a productive, performant, and educational language to code, and the lack of a well-developed cell-based simulation system like CHASTE is the primary reason the thesis is not entirely Julian. 

The C++ code requires the CHASTE framework \cite{Mirams2013} to compile. If replication of the simulation pipeline in \autoref{chap:SMME} is required, CHASTE can be downloaded from \url{https://www.cs.ox.ac.uk/chaste/index.html} and used to compile the simulators in the \path{SMME/apps} folder of the SMME repo, available in \autoref{SMMEcode}. The simulators are managed from the python script fixtures available in \path{SMME/python_fixtures}. Python used for the SMME work is python3.

We also used the Java-based KNIME scripting framework to manage the execution of the bioinformatics pipeline for calling nucleosome positions in \autoref{chap:rys}. This made use of the KNIME4NGS package \cite{Hastreiter2017}. The KNIME workspace used is available in the HDD archive at \path{/knime_workspace}. This workspace includes a THiCweed analysis pipeline \cite{Agrawal2017}, which is not presented in this thesis, and can be deleted or ignored. The pipeline requires both python2 and python3 environments, because danpos, the nucleosome position calling algorithm we used, is written in python2. danpos \cite{Chen2013} is no longer available in functioning form from the author's archive. We have uploaded a version that fixes execution errors to \path{https://github.com/mmattocks/danpos-fixed}.

\section{Computational cluster description and discussion}
\label{sec:cluster}
The computational cost of the model sampling performed in this thesis varies widely. The \hyperref[ssec:ICA]{ICA} models estimated in the \textit{rys} work of \autoref{chap:rys} are comparatively enormous problems with expensive solutions, and require more power to solve in a reasonable timeframe than is typically available in a molecular biology lab. On the other hand, the CMZ slice models used in \autoref{chap:CMZ} can be estimated in a day on typical higher-end PCs used for microscopy acquisition and the like. This thesis used two machines as the local cluster for the non-ICA problems. None of the problems made use of GPU processing, although some of them could possibly benefit from the substitution of CUDA arrays.

The first machine was an obsolete watercooled 4-core i5-4670K overclocked to 4.4 GHz, with 16 GB RAM. It has good thermal characteristics and maintains full clock under 100\% load at 60\textdegree C. This machine is adequate for estimating most of the \hyperref[chap:CNS]{CMZNicheSims.jl} models, on the datasets of this thesis, in less than a day. The CHASTE/SPSA approach presented in \autoref{chap:SMME} is slow; it takes 7-10 days to execute the required ~15000 iterates on this machine. The comparable model in \path{CMZNicheSims.jl} is the \path{Thymidine_Model}, which is the slowest of the simulators presented in \autoref{chap:CNS}. Generally, this machine will complete about 1500 iterates a day using this purpose-coded model in Julia. Direct comparison between SPSA and GMC estimation processes is difficult, but this suggests that the overhead from using the object-oriented spatial modelling framework CHASTE does not dominate computational cost. Still, given Julia's C++-equivalent performance and much higher coding productivity, it seems the purpose-built, functionally-coded model will usually be faster and cheaper to develop and execute, overall. In any case, for the types of relatively simple observational datasets collected from microscopy studies, lab machines will suffice; for larger bioinformatic problems like those tackled in \autoref{chap:rys}, this machine is best used as a combined master and fast calculation node in a cluster of 50+ dedicated calculation nodes. A particular limitation of older machines is the relatively low cap on the size of available RAM DIMMs. Generally, this prevents maximum parallelisation of \hyperref[chap:BBM]{BioBackgroundModels.jl} models across cores. Parallelisation is easier to achieve on more modern machines with more cores/threads and RAM. 

The second machine was a more modern 12-threaded i7-9750 in a Tongfang mobile chassis, clocked to 4.0 GHz, with 32 GB RAM. This machine is strongly thermally limited\footnote{This machine cooked its own motherboard during the thesis work, requiring replacement with a revised board less vulnerable to overheating. This can be partially overcome in Canadian winters by calculating outside.}, typically running at 3.4GHz and 90\textdegree C under full load. Still, the additional threads and RAM make higher-end laptops a useful calculation node. Machines like this can expect to estimate two or more of the \path{CMZNicheSims.jl} models simultaneously in less than a day, given similarly sized datasets. They can expect to supply about three 4-threaded workers for distributed \path{BioBackgroundModels.jl} or \hyperref[chap:BBM]{BioMotifInference.jl} jobs, with significantly better performance than the AWS nodes described below.

Both BBM.jl and BMI.jl assume that the user will want to distribute the job across a cluster of dissimilar workers. Because Julia has an extremely elegant Distributed package as part of its Base distribution, any type of julia \path{ClusterManager} with master-worker topology can be used. For this thesis, we chose to purchase compute time on the spot market at Amazon Web Services (AWS). A small module with helper functions for purchasing node time on this market this is supplied in \autoref{AWSWrangler}. The AMI image used in this thesis is available in the HDD archive. We used \path{c5a.24xlarge} nodes. These have 48 cores per socket, clocked at 2.8GHz, and 196 GB of RAM. Most AWS instances are slower than enthusiast hardware, but the sheer number of available cores and RAM makes them an effective way to quickly assemble a large worker pool. We usually used both \path{BBM.jl} and \path{BMI.jl} with 4 cores per worker, as the threading overhead decreases efficiency beyond this point. Some \path{CNS.jl} simulations can usefully use 24-36 threads, but these should be benchmarked. \path{Thymidine_Ensemble}s do noticeably poorly with large thread numbers, for instance. The model comparison performed in \autoref{chap:rys} required five \path{c5a.24xlarge} instances, added to the two machines described above. These ran for approximately two weeks (half a week for each of the sib and \textit{rys} observations sets and a further week for the combined set). Typical spot prices for these nodes are about USD $1.hr^{-1}$ at the time of writing, giving an approximate cost of USD 1700 for a problem of this size. On-demand pricing is typically 4-5 times this cost; obviously, it is preferable to use the spot market.

This presents the risk that nodes may be stopped at any time, either because the spot price exceeds the submitted maximum bid, or because AWS requires the compute power for the on-demand market \footnote{High Netflix viewing times seem to be associated with this.}. As a result, all of Julia algorithms available in this thesis back up to disk and are robust to dropped workers, power outages, and other kinds of interruption. To resume an interrupted calculation performed from one of the Julia analysis scripts in this thesis, simply execute it again.

Users of \path{BBM.jl} and \path{BMI.jl} clusters should be aware that observations data transfer to workers can take a very long time, particularly with the relatively slow connections provided by AWS. This presents an additional complexity to budgeting for the spot market: interruptions require the re-transfer of observations to each worker in turn. \path{BMI.jl} waits for observations to finish transferring to each worker before starting the next; this prevents the situation where attempting to transfer to all nodes simultaneously on the same connection prevents any of them from starting on the problem. Probably, for larger datasets and larger models, the master node should be one of the cloud devices, to allow the observations to be local to those machines.