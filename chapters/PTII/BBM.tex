\chapter{BioBackgroundModels.jl: Parallel training of Hidden Markov Model zoos of genomic background noise in Julia}
\label{ch:BBM}

\path{BioBackgroundModels.jl} is a pure Julia package intended to automate the optimization and selection of large numbers ("zoos") of Hidden Markov Models (HMMs), using sequence sampled from specified partitions of a given genome, on arbitrary collections of hardware. It is supplied with extensive utility functions for genomic sampling, high performance implementations of both the Baum-Welch and Churbanov-Winters algorithms for optimization of HMMs by expectation maximization (EM), as well as reporting functions for summarizing the results of the optimized model zoos. These tasks are necessary to select of models of genomic background noise, from which motif signals may be detected using a package such as \path{BioMotifInference.jl}.

HMMs generated by \path{BBM.jl} are standard $\geq$ 1 state HMMs that emit symbols corresponding to stretches of 1 or more base pairs. Following Down et al. \cite{Down2005}, the number of bases encoded by an HMM symbol is denoted by the ``order'' of the HMM. 0th order HMM emit 4 symbols, one for each of the genomic nucleotides, while 1\textsuperscript{st} order HMMs emit 16 symbols, one for each 2-base combination, and 2\textsuperscript{nd} order HMMs emit 64 symbols, one for each 3-base combination, and so on. It is expected that users will be interested in comparing the explanatory value of a variety of background HMM state numbers and orders in selecting appropriate background models. These can all be optimized in one batch; because high state and, to a lesser extent, high order numbers impose much more significant memory costs, the use of the Churbanov-Winters linear memory algorithm is strongly recommended. A basic load balancing system has been provided to allow users to prefer particular machines for some optimization jobs over others, which permits the efficient use of clusters of dissimilar hardware in the batch context.

\section{Genome partitioning and sampling}
The optimization of an HMM zoo is performed against a sample of a ``genome partition'', which defines a portion of a genome in relation to a genomic feature set. The required files to sample from a genome partition are:

\begin{itemize}
    \item A whole-genome sequence in valid FASTA format (eg. a .fna or other appropriate nucleotide sequence file).
    \item An index for the genomic sequence file (eg. an .fna.fai file)
    \item A genomic feature file, annotating the genomic sequence, in GFF3 format
\end{itemize}

At present, \path{BBM.jl} offers a default strategy of sampling without replacement from exonic, periexonic, and intergenic partitions of genomes. This partition scheme is intended to differentiate between genomic sequences that are likely to differ in terms of the appropriateness of HMMs of different orders as background models. That is, the selective pressure imposed on genomic sequence by the codon code (exonic partition) and other higher-order functional features of gene organization (periexonic partition) are likely to favour different sorts of background HMM model than the simpler repetitive structure of intergenic material.

Sampling functions should normally be accessed through the API provided to the user. Firstly, sampling jobs may be set up by using the \path{setup_sample_jobs} function, which uses user-supplied paths to the files listed above, as well as user-specified partitioning variables, to divide the supplied genome into the partitions described and set up channels for these sampling jobs. For example:

\begin{minted}[breaklines,
               mathescape,
               linenos,
               numbersep=5pt,
               frame=lines,
               framesep=2mm]{julia}
using BioBackgroundModels
#FILE PATHS
danio_gff_path = "Danio_rerio.GRCz11.94.gff3"
danio_genome_path = "GCA_000002035.4_GRCz11_genomic.fna"
danio_gen_index_path = "GCA_000002035.4_GRCz11_genomic.fna.fai"

#CONSTANTS FOR GENOMIC SAMPLING
const sample_set_length = Int64(4e6)
const sample_window_min = 10
const sample_window_max = 3000
const perigenic_pad = 500

channels = setup_sample_jobs(danio_genome_path, danio_gen_index_path, danio_gff_path, sample_set_length, sample_window_min, sample_window_max, perigenic_pad)
\end{minted}

Here, \path{sample_set_length} defines the overall number of bases to be sampled from each partition, while \path{sample_window_min} and \path{sample_window_max} define the minimum and maximum length of each individual sample to be taken, without replacement, from the partitions. \path{sample_set_length} should be chosen to supply both training and test sets for background model selection; ie. it should be twice the desired length of the observation set in training. Lastly, \path{perigenic_pad} defines the number of bases up- and down-stream of the first and last codon to consider, along with intronic material, as constituting the ``perigenic'' partition. This will include promoter elements, splicing signals, and the like.

Secondly, the partitions are sampled (in parallel, by worker, if desired), to produce dataframes of samples and relevant coordinate and strand orientation information:

\begin{minted}[breaklines,
    mathescape,
    linenos,
    numbersep=5pt,
    frame=lines,
    framesep=2mm]{julia}
using Distributed,Serialization
sample_output = "samples"
worker_pool = addprocs(3)
sample_record_dfs=execute_sample_jobs(channels, worker_pool)
serialize(sample_output, sample_record_dfs)
\end{minted}

Here, one worker per partition is used to sample; lower numbers of workers will also work while larger numbers are of no benefit. In this example, the sample dataframes are serialized to the file ``samples'' for later use; they can also be used directly as described below.

\section{Optimizing BHMMs by EM algorithms}
Background HMM optimization by the expectation-maximization algorithms is performed by a similar two-step setup-and-execute functional pattern as genome sampling. Firstly, \path{setup_EM_jobs!} is used to transform supplied ``job IDs'' and the observations supplied by genome sampling into sets of EM chains to be elongated by the EM algorithm. \path{BBM.jl} ``job IDs'' take the form of \path{Chain_ID} structs, whose fields specify a string denoting the partition that forms the observations for the optimization, the number of states in the HMM, the order of the HMM's emitted symbols, and an integer denoting the replicate number of the chain. In general, for any given partition, number of states, and emission order, three or more replicates are advised, to confirm that the converged values of any particular HMM are likely to be near the global optimum. That is, if replicates of some job ID chain all terminate with similar parameter values (see \autoref{sec:BBMdisplay} to examine this using \path{BBM.jl}), one such chain may reasonably be selected as a representative of the likely global optimum present in this region of the parameter space.

The helper function \path{split_obs_sets} is used to divide the genomic sample dataframes generated by the sampling API, described above, into training and test sets for optimization. The training set is supplied for the use of the EM algorithms, while the test set is used solely to calculate the final likelihoods of optimized models.

By default, \path{BBM.jl} initialises new HMM chains by sampling randomly from possible emission vectors, with strong priors on transition matrices with robust autotransition. Generally speaking, this tends to prevent chains from being optimized to trivial local minima with very short state residency times. This reflects our exclusive interest in those HMMs whose states represent stretches of nucleotides with shared biological significance; HMMs which tend to have state residency emission lengths of less than a few bases are of questionable biological relevance. If desired, the user may supply their own initialisation function as the \path{setup_EM_jobs!} keyword argument \path{init_function}, which defaults to \path{autotransition_init}. The default function should serve as the template for such user-defined initialisation functions.

\path{BBM.jl} operation is designed to be robust to most kinds of interruption, and the \path{setup_EM_jobs!} function may be used safely on existing sets of EM chains generated by BBM. This allows for resumption from interruption, reconfiguration of hardware, decreasing the desired chain termination threshold, and so on. The dict containing existing serialized chains, if any, should be supplied to the setup function as the \path{chains} keyword variable to allow this.

\begin{minted}[breaklines,
    mathescape,
    linenos,
    numbersep=5pt,
    frame=lines,
    framesep=2mm]{julia}
using BioBackgroundModels, DataFrames, Distributed, Serialization

#FILE PATHS
hmm_output = "chains"
sample_output = "samples"

#JOB CONSTANTS
const replicates = 3 #repeat optimisation from this many seperately initialised samples from the prior
const Ks = [1,2,4,6] #state #s to test
const order_nos = [0,1,2] #DNA kmer order #s to test
const delta_thresh=1e-5 #stopping/convergence criterion (log probability difference btw subsequent EM iterates)
const max_iterates=50000

#PROGRAMATICALLY GENERATE Chain_ID Vector
job_ids=Vector{Chain_ID}()
for (obs_id, obs) in training_sets, K in Ks, order in order_nos, rep in 1:replicates
    push!(job_ids, Chain_ID(obs_id, K, order, rep))
end

#SPLIT GENOME SAMPLES INTO TRAINING AND TEST SETS
sample_dfs = deserialize(sample_output)
training_sets, test_sets = split_obs_sets(sample_dfs)

#INTIIALIZE HMMS
if isfile(hmm_output) #if some results have already been collected, load them
    hmm_results_dict = deserialize(hmm_output)
else #otherwise, pass a new results dict
    hmm_results_dict = Dict{Chain_ID,Vector{EM_step}}()
end

em_jobset = setup_EM_jobs!(job_ids, training_sets; delta_thresh=delta_thresh, chains=hmm_results_dict)
\end{minted}

Once the EM jobset is set up, it is splatted into the \path{execute_EM_jobs!} function with any valid worker pool and the path for the Dict of EM chains to be serialized. All workers in the pool should be in \path{:master_worker} topology with the master process. \path{BBM.jl} workers will obtain new chains to elongate by EM until no more are available; the number of jobs, therefore, defines the useful upper limit for the number of workers in the pool.

Load balancing is achieved by using \path{BBM.jl}'s LoadConfig struct, whose fields define acceptable ranges of states, orders, and optional vectors of blacklisted and whitelisted \path{Chain_IDs}. A Dict of LoadConfigs keyed by Integer defines the appropriate LoadConfig to use for any given worker's integer id (ie. the output of \path{Distributed.myid()}).

The results of EM optimizations are serialized as they are ``pulled off the wire'', so that they cannot be lost by the algorithm being interrupted, network failure etc. Consequently, \path{execute_EM_jobs!} does not return anything that needs to be serialized for subsequent analyses; results are always located at the path provided to the function and may be freely inspected by the tools described in \autoref{sec:BBMdisplay} at any time during or after the optimization.

An example of \path{execute_EM_jobs!} usage, following from the setup example given above:

\begin{minted}[breaklines,
    mathescape,
    linenos,
    numbersep=5pt,
    frame=lines,
    framesep=2mm]{julia}
#SETUP LOAD BALANCING
local_config=LoadConfig(1:6,0:2)
remote_config=LoadConfig(1:4,0:1)

load_dict=Dict{Int64,LoadConfig}()

worker_pool=addprocs(no_local_processes, topology=:master_worker)
for worker in worker_pool
    load_dict[worker]=local_config
end

remote_pool=addprocs([(remote_machine,no_remote_processes)], tunnel=true, topology=:master_worker)

for worker in remote_pool
    load_dict[worker]=remote_config
end

worker_pool=vcat(worker_pool, remote_pool)

#EXECUTE EM JOBS
execute_EM_jobs!(worker_pool, em_jobset..., hmm_output; load_dict=load_dict, delta_thresh=delta_thresh, max_iterates=max_iterates)
\end{minted}

\section{BHMM analysis and display}
\label{sec:BBMdisplay}
A report generation and display API is provided in order to analyse the results of optimizing a zoo of background HMMs using \path{BBM.jl}. One function, \path{generate_reports}, serves to prepare all of the available reports on the zoo. Its use is straightforward:

\begin{minted}[breaklines,
    mathescape,
    linenos,
    numbersep=5pt,
    frame=lines,
    framesep=2mm]{julia}
chains=deserialize(hmm_output)
sample_dfs = deserialize(sample_output)
training_sets, test_sets = split_obs_sets(sample_dfs)

report_folders=generate_reports(chains, test_sets)
serialize(survey_folders, report_folders) #save reports
\end{minted}

\path{generate_reports} creates a Dict of \path{Report_Folder}s, keyed by partition id string. Each \path{Report_Folder} contains a \path{Partition_Report} with information about all HMMs trained on that partition, a \path{Replicate_Report} which enables the comparison of the replicates of the best model available for the partition, and a Dict of \path{Chain_Report}s keyed by \path{Chain_ID}, which give specific information concerning each chain produced for the partition. Examples of report output follow.

